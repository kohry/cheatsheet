# 베이지안 decsition

# 필요한 패키지
e1071 - 베이지안
caret - 파티셔닝

# sampling
x <- 1:12
sample(x, replace = T)

# Caret package

Classification And REgression Training

caret 패키지의 createDataParititon()은 특정비율로 파티셔닝 할때 편함. 종속변수의 요인별 비율만큼 층화랜덤추출을 기본지원.
예를들어, 훈련데이터와 테스트데이터를 같은 목적의 데이터로 나누려면 이렇게 비율로 쪼개줄수 있다.

# naive bayes training

model = naiveBayes(AHD ~ ., data = train)

Naive Bayes Classifier for Discrete Predictors

Call:
naiveBayes.default(x = X, y = Y, laplace = laplace)

A-priori probabilities:
Y
       No       Yes 
0.5399061 0.4600939 

Conditional probabilities:
     X
Y         [,1]     [,2]
  No  145.7391 87.80891
  Yes 160.1735 88.22289

     Age
Y         [,1]     [,2]
  No  52.05217 9.789758
  Yes 57.12245 7.500193

     Sex
Y          [,1]      [,2]
  No  0.5304348 0.5012570
  Yes 0.8163265 0.3892085

     ChestPain
Y     asymptomatic nonanginal nontypical    typical
  No    0.26086957 0.41739130 0.24347826 0.07826087
  Yes   0.73469388 0.14285714 0.07142857 0.05102041

     RestBP
Y         [,1]     [,2]
  No  130.5043 16.71924
  Yes 134.8163 20.16905

     Chol
Y         [,1]     [,2]
  No  243.1391 51.17169
  Yes 251.4898 50.96719

     Fbs
Y          [,1]      [,2]
  No  0.1217391 0.3284153
  Yes 0.1224490 0.3294890

     RestECG
Y         [,1]      [,2]
  No  0.773913 0.9738769
  Yes 1.204082 0.9734537

     MaxHR
Y         [,1]     [,2]
  No  158.4609 19.83055
  Yes 139.1020 22.39914

     ExAng
Y          [,1]      [,2]
  No  0.1652174 0.3730019
  Yes 0.5204082 0.5021519

     Oldpeak
Y          [,1]      [,2]
  No  0.5913043 0.8054164
  Yes 1.4918367 1.2824488

     Slope
Y         [,1]      [,2]
  No  1.400000 0.6040797
  Yes 1.806122 0.5497546

     Ca
Y          [,1]      [,2]
  No  0.2894737 0.6479878
  Yes 1.2142857 0.9870293

     Thal
Y          fixed     normal reversable
  No  0.02631579 0.80701754 0.16666667
  Yes 0.09183673 0.23469388 0.67346939

# predict
predict(model)

> table(test$AHD, p)
     p
      No Yes
  No  43   6
  Yes  8  33


# table +
table(test$AHD, p)
confusionMatrix(test$AHD, p)

# confusion matrix
> confusionMatrix(test$AHD, p)
Confusion Matrix and Statistics

          Reference
Prediction No Yes
       No  43   6
       Yes  8  33
                                          
               Accuracy : 0.8444          
                 95% CI : (0.7528, 0.9123)
    No Information Rate : 0.5667          
    P-Value [Acc > NIR] : 1.734e-08       
                                          
                  Kappa : 0.6852          
 Mcnemar's Test P-Value : 0.7893          
                                          
            Sensitivity : 0.8431          
            Specificity : 0.8462          
         Pos Pred Value : 0.8776          
         Neg Pred Value : 0.8049          
             Prevalence : 0.5667          
         Detection Rate : 0.4778          
   Detection Prevalence : 0.5444          
      Balanced Accuracy : 0.8446          
                                          
       'Positive' Class : No   

여기서, 
실제 맞는걸 맞다고 한경우, True Positive
틀린걸 맞다고 하는 재수없는 경우, False positive
맞는걸 틀리다고 하는 답답한 경우, False Negative
틀린걸 틀리다고 하는, True Negative

정확도 (Accuracy) : 전체중 바르게 예측한 비율
민감도 (Sensitivity) : 실제로 일어난것들중 제대로 예측한 확률.
특이도 (Specificity) : 안일어난건데 안일어났다고 한 비율

12월 - ex
8월 - gi
5월 - adp

## decision tree



df<-read.csv('http://www-bcf.usc.edu/~gareth/ISL/Heart.csv')
str(df)

install.packages("tree")
library(tree)


library(caret)
set.seed(1000) #reproducability setting
intrain<-createDataPartition(y=df$AHD, p=0.7, list=FALSE) 
train<-df[intrain, ]
test<-df[-intrain, ]

?tree
mod <- tree(AHD~., data = train)
plot(mod)
text(mod)
predict(mod, test)

?cv.tree

cv.trees<-cv.tree(mod, FUN=prune.misclass ) # for classification decision tree
plot(cv.trees)

prune.trees <- prune.misclass(mod, best=6)  # for regression decision tree, use prune.tree function
plot(prune.trees)
text(prune.trees, pretty=0)

library(e1071)
treepred <- predict(prune.trees, test, type='class')
confusionMatrix(treepred, test$AHD)



## neural network

install.packages("neuralnet")
library(neuralnet)


samplesize = 0.6 * nrow(df)
samplesize
index = sample(nrow(df), size = samplesize)
index
train <- df[index,]
test <- df[-index,]


head(train)
head(test)

str(train)
str(test)
str(df)

max = apply(df, 2, max)
min = apply(df, 2, min)
scaled = as.data.frame(scale(c(df$Age, df$Sex, df$Chol)))

NN = neuralnet(Sex ~ Age + Chol, train, hidden = 3, linear.output = T)

## knn

library(class) 
library(dplyr)

select

select keeps only the variables you mention.

food <- data.frame(ingredient = c("apple", "bacon", "banana", "carrot",
                                  "celery", "cheese", "cucumber", "fish",
                                  "grape", "green bean", "lettuce",
                                  "nuts", "orange", "pear","shrimp"
),
sweetness = c(10,1,10,7,3,1,2,3,8,3,1,3,7,10,2),
crunchiness = c(9,4,1,10,10,1,8,1,5,7,9,6,3,7,3),
class = c("Fruits","Proteins","Fruits","Vegetables",
          "Vegetables","Proteins","Vegetables",
          "Proteins","Fruits","Vegetables",
          "Vegetables","Proteins","Fruits",
          "Fruits","Proteins"))
food

tomato <- data.frame(ingredient = "tomato",
                     sweetness = 6,
                     crunchiness = 4)
tomato


install.packages("dplyr")

library(class) 
library(dplyr)

tmt <- knn(select(food, sweetness, crunchiness), 
           select(tomato,sweetness, crunchiness), 
           food$class, k=1)
tmt

unknown <- data.frame(ingredient = c("grape", "green bean", "orange","tomato"),
                      sweetness = c(8,3,7,6),
                      crunchiness = c(5,7,3,4))
unknown

pred <- knn(select(food, sweetness, crunchiness), 
            select(unknown,sweetness, crunchiness), 
            food$class, k=3)
pred

## etc

prop.table()
비율별로 꾸미기

gmodels 패키지안에 CrossTable()
이원교차표. 교차표는 한값이 다른값을 변경하는지 살펴볼수있음



