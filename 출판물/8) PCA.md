# Principal Component Analysis

## 변수가 너무 많을때.

<<<<그림>>>>

1차원에서 2차원으로 확장하는건 쉽습니다.

2차원에서 3차원으로 확장하는것도 조금의 상상을 통하면 쉽습니다.

3차원에서 4차원으로 가는순간 인간의 상상력은 한계를 넘어버립니다.

하지만, 어떠한 사건에 영향을 미치는 데이터들의 요소들은 3개를 넘어가는 일이 다반사입니다. 

초끈이론으로 13차원 분석을 하기 전에, 우리는 그냥 주요한 몇개의 특성만을 다시 추려내 데이터를 저차원 데이터로 변환해볼수 있습니다.

## 얼굴몰아주기
사진찍을때 얼굴몰아주기라는 것이 있었습니다. PCA는 항목들(독립변수)의 고루 퍼져있는 상태를 PC1부터 몰빵합니다. 그렇다면 거의 많은 것들은 PC1이라는 새로운 혼합된 독립변수로 설명이 거의 가능하고, 모자라면 PC2까지 가져다 쓰면됩니다. PC3, PC4.. 이렇게 갈때마다 중요도가 떨어지기 때문에 내다 버려도 상관없게 됩니다.

## 어떻게 줄이나?
분산이 제일 많은 항목을 하나 골라 이를 PC1로 두고 이에 수직인 축들을 PC2, PC3...로 나눕니다. PC는 차원수만큼 존재하게 됩니다. 다만, 뒤로갈수록 별로 영향력이 크지않은 항목입니다. 

## 범죄 데이터로 살짝 맛보기

이번에의 데이터는 USArrests 데이터셋입니다. 이를 다음과 같은 코드로 간단하게 PCA를 돌려볼수 있습니다.

```
p <- prcomp(USArrests, scale = TRUE)
```

이 결과를 바로 plot으로 찍어보면,

```
summary(p)

Importance of components%s:
                          PC1    PC2     PC3     PC4
Standard deviation     1.5749 0.9949 0.59713 0.41645
Proportion of Variance 0.6201 0.2474 0.08914 0.04336
Cumulative Proportion  0.6201 0.8675 0.95664 1.00000

plot(p)
```

<<prcomp그래프 넣는다.>>

실제로 모든 USArrests의 변동이 한 첫번째 PC에 몰빵된것을 알수있습니다. 만약 모델을 짜려면,
y = ax1 + bx2 + cx3 + dx4 + e
같은 복잡한 식이 아닌
y = a * p + e
와같은 간단한식으로도 대부분 변수들의 움직임이 설명될수 있게 짤수 있을겁니다.

```
biplot(prcomp(USArrests, scale = TRUE))
```

위의 그래프를 통하면, 원래의 변수들이 어떤식으로 흩어져 있었는지도 확인할수 있습니다. 물론 차트는 제일 중요한 PC1과 그다음으로 중요한 PC2로 축들이 결정됩니다. 그이상의 우리 눈알에 보이지 않으니까요 ^.^

## 디시전트리로 이해하기
데이터를 주성분으로 표현하는 것은 결정 트리(decision tree)의 단계를 결정 단계를 밟아나가는 것과 유사합니다. 잘 만들어진 결정 트리는 첫번째 가지의 if-then-else가 가장 대표적인 질문이 되어야 하는데, 이것은 마치 PCA의 첫번째 주성분이 가장 엔트로피를 감소시키는 차원을 고르는 것과 같습니다.
분산이 낮아진다는것은 엔트로피가 감소하는것을 의미. 예측가능한 상태라는것. 데이터의 분포를 주성분으로 나타내면 성분의 차수가 낮아질수록 분산감소. 즉 하나로만 설명할수 있다는것.

## 참조 및 시각화 사이트

http://setosa.io/ev/principal-component-analysis/

# 성분이 서로 독립적이지 않을때도 쓴다!